{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T08:36:26.813781300Z",
     "start_time": "2023-11-07T08:36:21.499772200Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import pickle\n",
    "import csv\n",
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "from transformers.optimization import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T08:36:27.899954300Z",
     "start_time": "2023-11-07T08:36:26.813781300Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.set_device(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T08:36:27.905491600Z",
     "start_time": "2023-11-07T08:36:27.899954300Z"
    }
   },
   "outputs": [],
   "source": [
    "# set dimension of interest\n",
    "# dim = 'romance'\n",
    "dims = [\n",
    "        'social_support',\n",
    "        'conflict',\n",
    "        'trust',\n",
    "        'fun',\n",
    "        'similarity',\n",
    "        'identity',\n",
    "        'respect',\n",
    "        'romance',\n",
    "        'knowledge',\n",
    "        'power'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T08:36:27.910880Z",
     "start_time": "2023-11-07T08:36:27.905491600Z"
    }
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = 'data/%s'%dim # directory of where the train/test/dev files are stored\n",
    "# OUTPUT_DIR = 'weights/BERT/%s' %dim # where the model weights will be stored\n",
    "BERT_MODEL = 'bert-base-cased' # BERT model type\n",
    "CACHE_DIR = 'cache/' # where BERT will look for pre-trained models to load parameters from\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "OUTPUT_MODE = 'classification'\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T08:36:30.098882Z",
     "start_time": "2023-11-07T08:36:27.910880Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL,cache_dir=CACHE_DIR, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T08:45:01.684752900Z",
     "start_time": "2023-11-07T08:44:09.574646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "e5jcml5: You seem like you thought on this for a little too long...\n",
      "\tsocial_support:\t0.077\n",
      "\tconflict:\t0.013\n",
      "\ttrust:\t0.025\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.061\n",
      "\tidentity:\t0.003\n",
      "\trespect:\t0.011\n",
      "\tromance:\t0.001\n",
      "\tknowledge:\t0.003\n",
      "\tpower:\t0.011\n",
      "\n",
      "e5hxhex: It evolved to adapt to the crushing pressure and oppressive darkness.\n",
      "\tsocial_support:\t0.009\n",
      "\tconflict:\t0.343\n",
      "\ttrust:\t0.028\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.212\n",
      "\tidentity:\t0.008\n",
      "\trespect:\t0.025\n",
      "\tromance:\t0.000\n",
      "\tknowledge:\t0.801\n",
      "\tpower:\t0.011\n",
      "\n",
      "e5a5d7h: There goes my theory that everyone who's blonde is a royal ^^/s\n",
      "\tsocial_support:\t0.013\n",
      "\tconflict:\t0.757\n",
      "\ttrust:\t0.026\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.004\n",
      "\tidentity:\t0.004\n",
      "\trespect:\t0.001\n",
      "\tromance:\t0.001\n",
      "\tknowledge:\t0.510\n",
      "\tpower:\t0.011\n",
      "\n",
      "e5y8n53: Because the thin blue line protects itself first and foremost. \n",
      "\tsocial_support:\t0.123\n",
      "\tconflict:\t0.015\n",
      "\ttrust:\t0.027\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.046\n",
      "\tidentity:\t0.032\n",
      "\trespect:\t0.006\n",
      "\tromance:\t0.001\n",
      "\tknowledge:\t0.978\n",
      "\tpower:\t0.011\n",
      "\n",
      "e6wfpic: Theyâ€™re known for chocking aggressive dogs and dingos to death. \n",
      "\tsocial_support:\t0.003\n",
      "\tconflict:\t0.987\n",
      "\ttrust:\t0.027\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.003\n",
      "\tidentity:\t0.003\n",
      "\trespect:\t0.007\n",
      "\tromance:\t0.000\n",
      "\tknowledge:\t0.110\n",
      "\tpower:\t0.011\n",
      "\n",
      "e5it410: Why do people have to be such assholes? Who raised these people?\n",
      "\tsocial_support:\t0.010\n",
      "\tconflict:\t0.990\n",
      "\ttrust:\t0.025\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.003\n",
      "\tidentity:\t0.014\n",
      "\trespect:\t0.003\n",
      "\tromance:\t0.000\n",
      "\tknowledge:\t0.002\n",
      "\tpower:\t0.011\n",
      "\n",
      "e62nvq5: Dog: \"I see you want to play! I too want to play!\"\n",
      "\tsocial_support:\t0.038\n",
      "\tconflict:\t0.014\n",
      "\ttrust:\t0.025\n",
      "\tfun:\t0.024\n",
      "\tsimilarity:\t0.018\n",
      "\tidentity:\t0.003\n",
      "\trespect:\t0.012\n",
      "\tromance:\t0.001\n",
      "\tknowledge:\t0.005\n",
      "\tpower:\t0.011\n",
      "\n",
      "e5u3kq8: I thought they were for busting out teeth American History X style\n",
      "\tsocial_support:\t0.005\n",
      "\tconflict:\t0.989\n",
      "\ttrust:\t0.026\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.006\n",
      "\tidentity:\t0.003\n",
      "\trespect:\t0.003\n",
      "\tromance:\t0.000\n",
      "\tknowledge:\t0.025\n",
      "\tpower:\t0.011\n",
      "\n",
      "e5yxdo8: We're at the point where new people don't remember unidan.... that's scary\n",
      "\tsocial_support:\t0.003\n",
      "\tconflict:\t0.359\n",
      "\ttrust:\t0.026\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.033\n",
      "\tidentity:\t0.062\n",
      "\trespect:\t0.002\n",
      "\tromance:\t0.000\n",
      "\tknowledge:\t0.030\n",
      "\tpower:\t0.011\n",
      "\n",
      "e6x2blc: Thnx. Seems crowded. I hope the 1.0 will allow get closer...\n",
      "\tsocial_support:\t0.102\n",
      "\tconflict:\t0.041\n",
      "\ttrust:\t0.028\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.033\n",
      "\tidentity:\t0.013\n",
      "\trespect:\t0.007\n",
      "\tromance:\t0.000\n",
      "\tknowledge:\t0.046\n",
      "\tpower:\t0.011\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "## load sentences\n",
    "file = open('C:\\\\Users\\\\fonta\\\\PycharmProjects\\\\10_dimensions_classifier\\\\out\\\\reddit_samples.txt')\n",
    "comments = json.load(file)\n",
    "scoring = {}\n",
    "for id in comments:\n",
    "    sentence = comments[id]\n",
    "    scoring.update({sentence: {}})\n",
    "    print(f'\\n{id}: {sentence}')\n",
    "    for dim in dims:\n",
    "        DATA_DIR = 'data/%s'%dim\n",
    "        OUTPUT_DIR = 'weights/BERT/%s' %dim\n",
    "        # load pretrained BERT model for specific dimension\n",
    "        output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
    "        model.load_state_dict(torch.load(output_model_file))\n",
    "        model.to(device)\n",
    "        tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=False)\n",
    "\n",
    "        input_ids = torch.tensor([tokenizer.encode(sentence,add_special_tokens=True)]).cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)[0]\n",
    "            score = torch.softmax(outputs,1)\n",
    "        scoring[sentence].update({dim: score[0,1].item()})\n",
    "        print(\"\\t%s:\\t%1.3f\" % (dim,score[0,1].item()))\n",
    "\n",
    "with open('C:\\\\Users\\\\fonta\\\\PycharmProjects\\\\10_dimensions_classifier\\\\out\\\\reddit_scoring.txt', 'w') as convert_file:\n",
    "     convert_file.write(json.dumps(scoring))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
