{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T11:09:49.770830500Z",
     "start_time": "2023-11-07T11:09:46.134526900Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler, TensorDataset)\n",
    "from torch.nn import CrossEntropyLoss, MSELoss\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "import pickle\n",
    "import csv\n",
    "from transformers import BertTokenizer,BertForSequenceClassification\n",
    "from transformers.optimization import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T11:09:49.815600100Z",
     "start_time": "2023-11-07T11:09:49.770830500Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# torch.cuda.set_device(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T11:09:49.822482500Z",
     "start_time": "2023-11-07T11:09:49.815600100Z"
    }
   },
   "outputs": [],
   "source": [
    "# set dimension of interest\n",
    "# dim = 'romance'\n",
    "dims = [\n",
    "        'social_support',\n",
    "        'conflict',\n",
    "        'trust',\n",
    "        'fun',\n",
    "        'similarity',\n",
    "        'identity',\n",
    "        'respect',\n",
    "        'romance',\n",
    "        'knowledge',\n",
    "        'power'\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T11:09:49.827369400Z",
     "start_time": "2023-11-07T11:09:49.822482500Z"
    }
   },
   "outputs": [],
   "source": [
    "# DATA_DIR = 'data/%s'%dim # directory of where the train/test/dev files are stored\n",
    "# OUTPUT_DIR = 'weights/BERT/%s' %dim # where the model weights will be stored\n",
    "BERT_MODEL = 'bert-base-cased' # BERT model type\n",
    "CACHE_DIR = 'cache/' # where BERT will look for pre-trained models to load parameters from\n",
    "\n",
    "num_labels = 2\n",
    "\n",
    "OUTPUT_MODE = 'classification'\n",
    "CONFIG_NAME = \"config.json\"\n",
    "WEIGHTS_NAME = \"pytorch_model.bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T11:09:51.885810700Z",
     "start_time": "2023-11-07T11:09:49.827369400Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = BertForSequenceClassification.from_pretrained(BERT_MODEL,cache_dir=CACHE_DIR, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-07T11:10:05.574949900Z",
     "start_time": "2023-11-07T11:09:51.854551200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1: Thnx. Seems crowded. I hope the 1.0 will allow get closer...\n",
      "\tsocial_support:\t0.102\n",
      "\tconflict:\t0.041\n",
      "\ttrust:\t0.028\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.033\n",
      "\tidentity:\t0.013\n",
      "\trespect:\t0.007\n",
      "\tromance:\t0.000\n",
      "\tknowledge:\t0.046\n",
      "\tpower:\t0.011\n",
      "\n",
      "2: Look, Dave, I know that you’re sincere and that you’re trying to do a competent job, and that you’re trying to be helpful, but I can assure the problem is with the AO-units, and with your test gear\n",
      "\tsocial_support:\t0.716\n",
      "\tconflict:\t0.018\n",
      "\ttrust:\t0.030\n",
      "\tfun:\t0.025\n",
      "\tsimilarity:\t0.022\n",
      "\tidentity:\t0.003\n",
      "\trespect:\t0.003\n",
      "\tromance:\t0.000\n",
      "\tknowledge:\t0.013\n",
      "\tpower:\t0.011\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "sentences = {\n",
    "    1: \"Thnx. Seems crowded. I hope the 1.0 will allow get closer...\",\n",
    "    2: \"Look, Dave, I know that you’re sincere and that you’re trying to do a competent job, and that you’re trying to be helpful, but I can assure the problem is with the AO-units, and with your test gear\"\n",
    "    }\n",
    "\n",
    "scoring = {}\n",
    "for id in sentences:\n",
    "    sentence = sentences[id]\n",
    "    scoring.update({sentence: {}})\n",
    "    print(f'\\n{id}: {sentence}')\n",
    "    for dim in dims:\n",
    "        DATA_DIR = 'data/%s'%dim\n",
    "        OUTPUT_DIR = 'weights/BERT/%s' %dim\n",
    "        # load pretrained BERT model for specific dimension\n",
    "        output_model_file = os.path.join(OUTPUT_DIR, WEIGHTS_NAME)\n",
    "        model.load_state_dict(torch.load(output_model_file))\n",
    "        model.to(device)\n",
    "        tokenizer = BertTokenizer.from_pretrained(BERT_MODEL, do_lower_case=False)\n",
    "\n",
    "        input_ids = torch.tensor([tokenizer.encode(sentence,add_special_tokens=True)]).cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids)[0]\n",
    "            score = torch.softmax(outputs,1)\n",
    "        scoring[sentence].update({dim: round(score[0,1].item(), 3)})\n",
    "        print(\"\\t%s:\\t%1.3f\" % (dim,score[0,1].item()))\n",
    "\n",
    "# with open('C:\\\\Users\\\\fonta\\\\PycharmProjects\\\\10_dimensions_classifier\\\\out\\\\manual_scoring.txt', 'w') as convert_file:\n",
    "#      convert_file.write(json.dumps(scoring))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
